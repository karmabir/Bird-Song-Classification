# Bird-Song-Classification

# Comparative Analysis of Deep Learning Models for Bird Song Classification

Identifying bird songs is essential in monitoring birds and their behaviour without physically interacting with them and disturbing their natural habitat. The field of audio classification is a recently emerging area of research that is mostly interpreted using supervised machine learning techniques. In recent studies, image classification methods and algorithms have shown great promise in this field. This paper analyses three different deep-learning models which are ANN, CNN and RNN-LSTM and compares these models comprehensively with the above-mentioned models. Metrics such as precision, recall and F1-score have been taken into consideration to decide which model among the three models is best for audio classification. The audio files are converted into Mel-Spectrogram to obtain MFCC’s and are fed into the models. It is observed that RNN-LSTM provides the highest accuracy followed by CNN and ANN respectively. This comprehensive study of existing models will help the researchers to choose the perfect model for audio classification for further research.

# Implementation

## Pre-processing:
To reduce the computer processing time, we chose five random species. For the files, the sample rate taken was 22050. These files were then trimmed to remove the blank spaces and the number of files for each bird was equal to remove the bias. The 2D visual representation was done by forming waveforms that show the oscillations of the amplitude as a function of time. Next, Short Time Fourier Transform (STFT) was performed, which was several Fourier Transforms at different intervals to preserve time information. This decomposes complex periodic sound into a sum of sine waves oscillating at different frequencies. It transforms both the y-axis (frequency) to log scale, and the “colour” axis (amplitude) to Decibels, which is approximately the log scale of amplitudes. So, in the end, this transformation gives a 3D representation of an audio file: the x-axis is time, the y-axis is frequency, and the colours (or z-axis) represent the amplitude. These Fourier transforms give us the spectrogram which gives us information about magnitude as a function of frequency and time. A Mel Scale on the y-axis was added to form Mel-Spectrogram. MFCCs catch the timbral and textural parts of sound. For removing MFCCs, we play out a Fourier Transform to move from the time area to the recurrence space so MFCCs are recurrence area highlights. The incredible benefit of MFCCs over spectrograms is that they rough the human hear-able framework, they attempt to display how we see a recurrence. In profound learning, have a few pieces of information that addresses the way human interaction sounds. The consequence of removing MFCCs is an MCC vector that contains between 13 to 40 coefficients. These coefficients are determined at each edge so we have a thought of how the MFCCs are developing after some time. We created a matrix of two dimensions for embedding into the RNN-LSTM model. For the CNN model, we added an axis to make it three dimensional and for the ANN model, that matrix was converted into a list.

## Models: 

1. Artificial Neural Networks: 
Created a five-layered model with 1344 neurons. In the first layer, a glorot uniform initializer has been used which draws samples from a uniform distribution within the limit. The initializer used in the second, third and fourth layer is l2 regularizer with the value of 0.01 has been implemented. While compiling, adam optimizer with the value of 0.01 has been taken.
2. Convolutional Neural Network
A five-layered model is created where four of the layers consists of activation function relu. The strides given is (2,2) for the first three layers. In the fourth layer, the dropout taken is 0.5. The total number of neurons taken is 176. The model has been compiled with the optimiser Adam with a learning rate of 0.0001. The Adam optimizer is a combination of AdaGard and RMSProp to give an optimized algorithm that can easily work on noisy data and its default value works well on various deep learning use cases.
3. Recurrent Neural Network - Long Short-term Memory
Created a five-layered model with 448 neurons. Consists of two LSTM layers, one dense layer and one output layer. For the LSTM layers, we have kept the return sequence as true and used kernel regularized L2 with a value of 0.05. The activation method is kept as relu for the dense layer and dropout as 0.01. Finally, for the output layer, the activation method is kept as softmax. For compiling the model adam optimizer the value of 0.0001 has been chosen.
After taking the input all the models were trained and tweaked and the final outputs were taken into consideration for the analysis.

## Conclusion: 
Certain pre-processing techniques were applied to retrieve the Mel-Frequency Coefficient data from the audio files which are adequate for the deep learning models to learn and classify the bird species on the basis of the input audio. RNN-LSTM provided the best output with an accuracy of 89.80% on the test data followed by CNN with an accuracy of 87.15% and then ANN with an accuracy of 81.15%. The reason for the RNN-LSTM model to outperform other models is due to its gated structure which helps in remembering information for long periods of time is practically their default behaviour. LSTMs are explicitly designed to avoid the long-term dependency problem which is not in the case of other models. As far as ANN and CNN are concerned both independently perform good depending upon the dataset. In this scenario, CNN performs better because the pictorial representation of the MFCC’s is well captured by the convolutional layers.
